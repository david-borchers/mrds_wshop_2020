---
title: Mark-recapture Distance Sampling
author: Centre for Research into Ecological and Environmental Modelling
subtitle: Workshop, 24-25 August 2020
date: Hidden Markol Line Transect Model Exercise
output: 
  pdf_document:
    number_sections: true
fontsize: 12pt
classoption: a4paper
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Specify whether answers are shown 
#answer <- TRUE
answer <- FALSE
```

# The Data

The data used in this exercise were gathered on a shipboard survey of beaked whales in the Alboran sea in 2008, from a vessel moving at about 3.5 m/s. Beaked whales have long dives and spend a lot of time underwater and unobservable. Focal follows of whales gave estimates of mean times available (at surface) and unavailable (diving) in a single dive cycle of 122 seconds and 1,580 seconds, respecively, with standard errors of these means of 9.62 seconds and 134.9 seconds, respectively.

A total of 81 groups were detected. The sightings data contain forward distance ($y$) as well as perpendicular distance ($x$), and also group size ($size$), Beaufort sea state ($bf$) and the height of the observer who made the detection ($ht$). We are going to focus on $x$ and $y$ only here. The survey was not a MRDS survey and so the data contains no duplicate data. 

\small
```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
library(hmltm)  # load the library
data("beaked.ship") # load the data (contained in the library)
knitr::kable(head(beaked.ship))
```
\normalsize

# Conventional Distance Sampling Analysis

Let's start by fitting a conventional disance sampling model to the data. To do that we first have to rename some columns in the data frame \texttt{beaked.ship} to comply with the conventions of the \texttt{Distance} package. We will put the data frame with the new column names in something called \texttt{bkdsdat}:

\small
```{r, echo=TRUE, eval=TRUE}
bkdsdat = beaked.ship
names(bkdsdat)[1:4] = c("Region.Label", "Area", "Sample.Label", "Effort")
names(bkdsdat)[which(names(bkdsdat)=="x")] = "distance"

knitr::kable(head(bkdsdat))
```
\normalsize

To fit a CDS model, we need the \texttt{Distance} package and we need to specify the units of the distances, transect lengths and areas, which are as follows:
\small
```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
library(Distance)
conversion.factor <- convert_units("meter", "kilometer", "Square kilometer")
```
\normalsize

Lets try a half-normal and a hazard-rate model, and compare their AICs:
\small
```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE, cache=TRUE}
bk.hn <- ds(data=bkdsdat, key="hn", adjustment=NULL,convert.units=conversion.factor)
bk.hr <- ds(data=bkdsdat, key="hr", adjustment=NULL,convert.units=conversion.factor)
knitr::kable(AIC(bk.hn,bk.hr))
```
\normalsize

The hazard-rate model is clearly preferable, so lets check that it seems adequate before proceeding:

\small
```{r, echo=TRUE, eval=TRUE, fig.height=3.5, fig.width=7}
cutpoints = seq(0,max(na.omit(bkdsdat$distance)),length=21)
par(mfrow=c(1,2))
plot(bk.hr, breaks=cutpoints, main="Hazard-rate model, beaked shipboard survey")
gof_ds(bk.hr)
```
\normalsize

That looks OK, so we will proceed using this model. We know that $g(0)$ is less than 1 (because of how long the whales dive), so the CDS estimates will be negatively biased, but lets look at the total abundance estimate anyway:

\small
```{r, echo=TRUE, eval=TRUE}
cds.N = c(est=bk.hr$dht$individuals$N$Estimate[3],
             lcl=bk.hr$dht$individuals$N$lcl[3],
             ucl=bk.hr$dht$individuals$N$ucl[3])
caption="CDS Beaked whale abundance estimates"
knitr::kable(t(round(as.data.frame(cds.N))),caption=caption)
```
\normalsize

# CDS with Correction Factors

Lets now calculate various correction factors and use them to ``correct'' for $g(0)<1$. We will do this for the simple (instantaneous) correction factor (whihc is just the proportion of time available), McLaren's factor, and Laake's factor. The \texttt{hmltm} package has functions that allow you to do this, given the mean times available and unavailable, and (optionally) estimates of the CVs of these times. McLaren's and Laake's factors require us to specify a forward distance at which animals are first detectable if they are available (we will call this \texttt{ymax}). 

Lets plot the data in $x$- and $y$- dimensions to figure out what a reasonable \texttt{ymax} would be

\small
```{r, echo=TRUE, eval=TRUE, fig.height=7}
xcuts = seq(0,max(na.omit(beaked.ship$x)),length=21)
ycuts = seq(0,max(na.omit(beaked.ship$y)),length=21)
layout(matrix(1:4,nrow=2))
hist(beaked.ship$x,breaks=xcuts,xlab="Perpendicular Distance",main="")
plot(beaked.ship$x, beaked.ship$y, pch="+",xlab="Perpendicular Distance",
     ylab="Forward Distance")
plot(0,0,xaxt="n",yaxt="n",bty="n",xlab="",ylab="",type="n")
hist(beaked.ship$y,breaks=xcuts,xlab="Forward Distance",main="")
```
\normalsize

From this, a \texttt{ymax} equal to 4,500 seems reasonable, so we will go with that. The \texttt{hmltm} package has a funtion \texttt{make.hmm.pars.from.Et} that constructs the HMM parameter object needed for inference from the mean times unavailable, mean time available, \texttt{ymax}, and survey platform speed:

\small
```{r, echo=TRUE, eval=TRUE}
ymax = 4500 # max forward distance can detect animal

Eu=1580 # mean time UNavailable -per dive cycle
Ea=122 # mean time available per dive cycle
spd = 3.5 # ship speed in m/s
# Calculate time in view, in minutes
tiv = ymax/spd / 60 # minutes in view

# Make availability parameters object
availpars = make.hmm.pars.from.Et(Eu=Eu, Ea=Ea, seEu=134.9, seEa=9.62)
```
\normalsize

From this we see that the time an animal is within detectable distance is `r round(tiv,2)` minutes, while the mean time UNavailable is `r round(Eu/60,2)` minutes and the mean time available is `r round(Ea/60,2)` minutes. Clearly $g(0)$ is going to be a lot less than 1 - but how much less? Lest calculate the naive estimate and ask McLaren and Laake. Package \texttt{hmltm} provides the functions \texttt{simple.a}, \texttt{mclaren.a}, and  \texttt{laake.a} to do the calculations, using objects like \texttt{availpars}, which we created above, as input:

\small
```{r, echo=TRUE, eval=TRUE}
simple.cf = simple.a(availpars)
laake.cf = laake.a(availpars,spd=spd, ymax=ymax)
mclaren.cf = mclaren.a(availpars,spd=spd, ymax=ymax)
# put them in a list for convenience
cf = data.frame(simple=simple.cf$mean, 
                mclaren=mclaren.cf$mean, 
                laake=laake.cf$mean) 
knitr::kable(round(cf,4),caption="Availability correction factors")
```

Lets use these to ``correct'' the CDS abundance estimates:

\normalsize
\small
```{r, echo=TRUE, eval=TRUE}
# Point estimates of total abundance and 95% CI with each correction method
simple.N = cds.N/cf[,"simple"]
mclaren.N = cds.N/cf[,"mclaren"]
laake.N = cds.N/cf[,"laake"]
Nests = t(data.frame(simple=round(simple.N),
                     mclaren=round(mclaren.N),
                     laake=round(laake.N)))
knitr::kable(Nests, caption="Corrected abundance estimates")
```

There are big differences between the methods! Which one to believe?

Lets take another look at the forward distance data and think about what this implies for the validity of each of the above correction methods:

\normalsize
\small
```{r, echo=TRUE, eval=TRUE, fig.height=3.5, fig.width=6}
hist(beaked.ship$y,breaks=xcuts,xlab="Forward Distance",main="")
```
\normalsize

## The simple correction factor

The simple method estimates the probability that a whale is available at a randomly chosen \textit{instant}. But here the whales are available (to some extent) for `r round(tiv,2)` minutes, which is very far from an instant, and so this simple method estimate is going to be way too small, and the corresponding ``corrected'' abundance estimate way too big.

## McLaren's correction factor

McLaren's method estimates the probability that a whale is available \texttt{at least once} while within detectable distance, but it does this in a way that can allow the probability to go above 1, and Laake's method fixes this problem, so Laake's method is better than McLaren's and we should prefer it to McLaren's.

## Laake's correction factor
Laake's method estimates the probability that a whale is available \texttt{at least once} while within detectable distance, and does so in a sensible way, but is this the appropriate correction factor?

Look at the forward distance plot and consider the fact that \textit{where} the animal becomes available between forward distance 0 and `r ymax` m. If it becomes available anywhere farther ahead than about 1,500 m, there is only a very small chance that it will be detected (we know this because very few detections occurred beyond about 1,500 m). So you would expect that an appropriate correction factor should be smaller than the probability that a whale is available \texttt{at least once}, and hence that Laake's correction factor is too big, and the associated abundance estimate too small.

# The Hidden Markov Line Transect Abundance Estimator

## The \texttt{hmltm} package

We will use the \texttt{hmltm} package to fit a hidden Markov line transect model to these survey data. The package is available on GitHub here [https://github.com/david-borchers/hmltm](https://github.com/david-borchers/hmltm). It was \textbf{not} written as a user-friendly general package, and has very little in the way of error traps and data checks. It has been used for a number of real analyses and so while I am pretty sure it is error free (or as error-free as a package with its level of use would typically be), I do not vouch for its ease of use, and nor do I guarantee that if you try it with new data that nothing will go wrong -- something probably will go wrong!


## Setting things up

To use the hidden Markov line transect (HMLT) method, we need first to spefify some key survey parameters:

\small
```{r, echo=TRUE, eval=TRUE}
# set survey and fitting parameters
spd=3.5 # observer speed
Wl=0 # left-truncation for perpendicular distance (none here)
W=4000 # right-truncatino for perpendicular distance
W.est=W # set perpendicular truncation distance for Horvitz-Thompson-like 
#         abundance estimator
ymax=4500 # maximum forward distance beyond which nothing could be detected

# specify survey parameters and put them in an object called `survey.pars`,
# using the hmltm funciton `make.survey.parsz
survey.pars=make.survey.pars(spd=3.5,Wl=0,W=4000,ymax=4500) 

# set some fitting parameters
# hessian=FALSDE says don't estimate Hessian matrix, and 
# nx=64 says use 64 cutpoints when integrating over the perp distance dimension
control.fit=list(hessian=FALSE,nx=64)

# set some parameters for the optimiser
# trace specifies how much to report as function is optimised
# maxit is maximum number of iterations when optimising
# see help(optim) for more
control.opt=list(trace=0,maxit=1000)
```


## Fitting a model

Now we fit the model with the function \texttt{est.hmltm}. You can choose from a number of detection hazard model forms (see the vignette that comes with the \texttt{hmltm} package) and depending on which model you use, you can specirfy dependence of the parameters in the $x$- and/or $y$-dimension on covariates. All the detection hazard forms assume that detection of an available animal that is at \textit{radial} distance zero (not perpendicular distance zero) are detected. Here we don't use any covariates. You also have to specify paramerter starting values, which is a bit of a pain, but the vignette that comes with the \texttt{hmltm} package) give a little guidance on this.

\normalsize
\small
```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
# specify model and starting parameter values and fit model
hfun="h.EP2x.0" # detection hazard model to use
models=list(y=NULL,x=NULL) # specify detection hazard model form (no covariates)

# You need to give starting values to the optimiser:
pars=c(0.45, 0.31, 7.7, 62.8) # detection hazard parameter starting values

# Point estimation:
bkEP2x.null=est.hmltm(beaked.ship,pars,hfun,models,survey.pars,availpars,
                      control.fit,control.opt,W.est=W.est)
```
\normalsize

## Checking goodness-of-fit

We can look at diagnostic plots similarly to the way we do for CDS models, but now we need to consider the goodness-of-fit in both the $x$- and the $y$-dimensions
\small
```{r, echo=TRUE, eval=TRUE, cache=TRUE, fig.height=6, fig.width=6, message=FALSE, warning=FALSE}
# Look at goodness of fit
par(mfrow=c(2,2))
bkEP2x.null.fx=fxfit.plot(bkEP2x.null,breaks=xcuts,type="prob",allx=FALSE)
bkEP2x.null.fy=fyfit.plot(bkEP2x.null,breaks=ycuts,allx=FALSE,nys=250)
bkEP2x.null.gof.x=hmmlt.gof.x(bkEP2x.null)
bkEP2x.null.gof.y=hmmlt.gof.y(bkEP2x.null,breaks=ycuts)
```
\normalsize

These plots and the associated Kolmogorov-Smirnov test statistics for the fits in the $x$- and the $y$-dimensions suggest that the model fits pretty well:

```{r, echo=TRUE, eval=TRUE, cache=TRUE}
gof_hmltm = data.frame(perp_dist=bkEP2x.null.gof.x$p.ks, 
                       forward_dist=bkEP2x.null.gof.y$p.ks)
knitr::kable(round(gof_hmltm,3),
             caption="Kolmogorov-Smirnov test statistics")
```

## Obtaining confidence intervals by bootstrap

Confidence intervals can be obtained by bootstrapping. For comparison with the correction factor methods used above, we treat the avaiability model as fixed and known here (\texttt{fixed.avail=TRUE}), but we need not do so in general. Similarly to other \texttt{hmltm} functions, you can get help on the use of the boostrap function \texttt{bs.hmltm} by typing \texttt{help(bs.hmltm)}.

\small
```{r, echo=TRUE, eval=FALSE, cache=TRUE}
set.seed(1)
bests = bs.hmltm(bkEP2x.null,B=50,hmm.pars.bs=availpars,bs.trace=0,report.by=10,
                 fixed.avail=TRUE,bsfile="./objects/beaked.bsests.RData")
```

The functions \texttt{bootsum} and \texttt{strat.estable} help you summarise the estimates from bootstraps in a convenient way:

```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
load("./objects/beaked.bsests.RData") # get bootstrap estimates file
bsum = bootsum(bests)
estable = strat.estable(bkEP2x.null$point$ests,bsum$cv)
hmltmNests = estable[3,c("N","N.lo","N.hi")]
names(hmltmNests) = colnames(Nests)
Nests = rbind(Nests,hmltm=hmltmNests)
knitr::kable(round(Nests),caption="Abundance estimates")
```
\normalsize

## Conclusion

The HMLT method abundance estimate falls between that from the simple (instantaneous) correction method and that from Laake's correction method -- as expected. Moreover, in scenarios in which the survey is not instantaneous (when the simple method is valid) and in which available animals are not certain to be detected when within detectable range (when Laake's method is valid), the correction factor methods are not appropriate.

In such cases the HMLT method provides a valid and rigorous method for estimating density and abundance. It uses more information than any of the correction factor methods (namely the forward distances), and it uses this with an assumption that available animals that are at \textit{radial} distance zero are detected with certainty - a much weaker assumption than that of CDS line transect methods. It comes with the usual maximum likelihood-based model selection and diagnostic tools. It does require forward distance data.

